{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 · Uncertainty Visualisation — Monte Carlo Dropout\n",
    "\n",
    "**Project:** SAM2 Lung Nodule Segmentation  \n",
    "**Date:** April 2025 (Phase 4)\n",
    "\n",
    "This notebook visualises the MC Dropout uncertainty maps and demonstrates their clinical value:\n",
    "\n",
    "1. MC Dropout inference on a CT slice\n",
    "2. Prediction spread across N stochastic passes\n",
    "3. Uncertainty heatmap (variance) overlay\n",
    "4. Uncertainty vs. prediction error correlation\n",
    "5. Reliability diagram (calibration)\n",
    "6. Threshold sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from data.dataset import build_dataset\n",
    "from evaluation.uncertainty_calibration import (\n",
    "    CalibrationAnalyzer,\n",
    "    brier_score,\n",
    "    entropy_auc,\n",
    "    expected_calibration_error,\n",
    "    reliability_diagram,\n",
    ")\n",
    "from models.mc_dropout import (\n",
    "    compute_uncertainty_stats,\n",
    "    entropy_from_samples,\n",
    "    mc_dropout_mode,\n",
    "    mc_predict,\n",
    ")\n",
    "from models.registry import get_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 · Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model (random weights if no checkpoint)\n",
    "CKPT = PROJECT_ROOT / \"runs\" / \"sam2_lung_seg_v1\" / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "model = get_model(\n",
    "    \"sam2_lung_seg\",\n",
    "    embed_dim=256,\n",
    "    num_heads=8,\n",
    "    attn_dropout=0.10,\n",
    "    proj_dropout=0.10,\n",
    "    encoder_frozen=False,\n",
    ")\n",
    "\n",
    "if CKPT.exists():\n",
    "    ckpt = torch.load(CKPT, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    epoch = ckpt.get(\"epoch\", \"?\")\n",
    "    vdice = ckpt.get(\"metrics\", {}).get(\"val_dice\", \"?\")\n",
    "    print(f\"Loaded checkpoint: epoch={epoch}  val_dice={vdice}\")\n",
    "else:\n",
    "    print(\n",
    "        \"No checkpoint found — using random weights (uncertainty patterns will be noise)\"\n",
    "    )\n",
    "\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Load a single slice\n",
    "ds = build_dataset(\"SYNTHETIC\", split=\"test\", mode=\"slice\", augment=False)\n",
    "sample = ds[0]\n",
    "img_t = sample[\"image\"].unsqueeze(0).to(device)  # (1, 1, H, W)\n",
    "msk_t = sample[\"mask\"].unsqueeze(0).to(device)  # (1, 1, H, W)\n",
    "\n",
    "H, W = img_t.shape[-2], img_t.shape[-1]\n",
    "print(f\"Image shape: {tuple(img_t.shape)}  |  Mask pixels: {msk_t.sum().item():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 · Prediction Spread across MC Samples\n",
    "\n",
    "We run N=20 stochastic forward passes and visualise the spread of predictions.\n",
    "High variance at slice boundaries indicates model uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 20\n",
    "\n",
    "all_preds = []\n",
    "with mc_dropout_mode(model):\n",
    "    for _ in range(N_SAMPLES):\n",
    "        with torch.no_grad():\n",
    "            logit = model(img_t)\n",
    "        all_preds.append(torch.sigmoid(logit).cpu().squeeze().numpy())\n",
    "\n",
    "all_preds_arr = np.stack(all_preds, axis=0)  # (N, H, W)\n",
    "mean_pred = all_preds_arr.mean(axis=0)\n",
    "std_pred = all_preds_arr.std(axis=0)\n",
    "\n",
    "img_np = img_t.cpu().squeeze().numpy()\n",
    "msk_np = msk_t.cpu().squeeze().numpy()\n",
    "\n",
    "# Show 6 individual predictions\n",
    "fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "fig.suptitle(\n",
    "    f\"MC Dropout — Individual Predictions (N={N_SAMPLES}, showing 6)\", fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "show_idx = np.linspace(0, N_SAMPLES - 1, 6, dtype=int)\n",
    "for col, idx in enumerate(show_idx):\n",
    "    axes[0, col].imshow(img_np, cmap=\"gray\")\n",
    "    axes[0, col].imshow(all_preds_arr[idx], cmap=\"hot\", alpha=0.6, vmin=0, vmax=1)\n",
    "    axes[0, col].set_title(f\"Pass #{idx+1}\", fontsize=9)\n",
    "    axes[0, col].axis(\"off\")\n",
    "\n",
    "    axes[1, col].imshow(all_preds_arr[idx], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    axes[1, col].set_title(f\"P={all_preds_arr[idx].mean():.3f}\", fontsize=9)\n",
    "    axes[1, col].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Overlay\", fontsize=9)\n",
    "axes[1, 0].set_ylabel(\"Prob map\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mc_individual_passes.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 · Mean Prediction and Uncertainty Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also get official mc_predict values\n",
    "mean_t, var_t = mc_predict(model, img_t, n_samples=25, mc_batch_size=5, sigmoid=True)\n",
    "mean_np = mean_t.cpu().squeeze().numpy()\n",
    "var_np = var_t.cpu().squeeze().numpy()\n",
    "entropy_np = entropy_from_samples(all_preds_arr)  # shape (H, W)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "fig.suptitle(\n",
    "    \"MC Dropout Uncertainty Decomposition (N=25)\", fontsize=13, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "titles = [\n",
    "    \"CT Slice\",\n",
    "    \"Ground Truth\",\n",
    "    \"Mean Prediction\",\n",
    "    \"Variance (Uncertainty)\",\n",
    "    \"Entropy\",\n",
    "]\n",
    "maps = [img_np, msk_np, mean_np, var_np, entropy_np]\n",
    "cmaps = [\"gray\", \"gray\", \"gray\", \"hot\", \"plasma\"]\n",
    "\n",
    "for ax, title, data, cmap in zip(axes, titles, maps, cmaps):\n",
    "    im = ax.imshow(data, cmap=cmap, vmin=0, vmax=(1 if cmap == \"gray\" else None))\n",
    "    ax.set_title(title, fontsize=10, fontweight=\"bold\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"uncertainty_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "unc_stats = compute_uncertainty_stats(var_t, (mean_t >= 0.5).float())\n",
    "print(\"Uncertainty Statistics\")\n",
    "for k, v in unc_stats.items():\n",
    "    print(f\"  {k:<25}: {v:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 · Uncertainty vs. Prediction Error Correlation\n",
    "\n",
    "A well-calibrated model should concentrate high uncertainty near prediction mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_pred = (mean_np >= 0.5).astype(float)\n",
    "errors = (binary_pred != msk_np).astype(float)  # 1 where wrong\n",
    "\n",
    "# Scatter: uncertainty per pixel vs. error\n",
    "unc_flat = var_np.flatten()\n",
    "err_flat = errors.flatten()\n",
    "\n",
    "# Sub-sample for speed\n",
    "idx_sub = np.random.choice(len(unc_flat), size=min(5000, len(unc_flat)), replace=False)\n",
    "unc_sub = unc_flat[idx_sub]\n",
    "err_sub = err_flat[idx_sub]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle(\"Uncertainty vs. Error Analysis\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Box plot\n",
    "axes[0].boxplot(\n",
    "    [unc_flat[err_flat == 0], unc_flat[err_flat == 1]],\n",
    "    labels=[\"Correct\", \"Error\"],\n",
    "    patch_artist=True,\n",
    "    boxprops=dict(facecolor=\"#4C72B0\", alpha=0.7),\n",
    ")\n",
    "axes[0].set_ylabel(\"MC Variance\")\n",
    "axes[0].set_title(\"Uncertainty by Correctness\")\n",
    "axes[0].grid(True, axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Histogram split\n",
    "axes[1].hist(\n",
    "    unc_flat[err_flat == 0],\n",
    "    bins=50,\n",
    "    alpha=0.7,\n",
    "    density=True,\n",
    "    label=\"Correct\",\n",
    "    color=\"#4C72B0\",\n",
    ")\n",
    "axes[1].hist(\n",
    "    unc_flat[err_flat == 1],\n",
    "    bins=50,\n",
    "    alpha=0.7,\n",
    "    density=True,\n",
    "    label=\"Error\",\n",
    "    color=\"#C44E52\",\n",
    ")\n",
    "axes[1].set_xlabel(\"MC Variance\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].set_title(\"Variance Distribution per Class\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Error map vs uncertainty map side by side\n",
    "axes[2].imshow(errors, cmap=\"Reds\", vmin=0, vmax=1, alpha=0.8)\n",
    "unc_norm = (var_np - var_np.min()) / (var_np.max() - var_np.min() + 1e-9)\n",
    "axes[2].contour(unc_norm, levels=[0.5], colors=[\"blue\"], linewidths=1.5)\n",
    "axes[2].set_title(\"Errors (red) + Unc contour (blue)\", fontsize=9)\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"uncertainty_error_correlation.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# AUROC of uncertainty as error detector\n",
    "auc = entropy_auc(unc_flat, err_flat)\n",
    "print(f\"Uncertainty AUROC (error detection): {auc:.4f}\")\n",
    "print(f\"(0.5=random, 1.0=perfect — target ≥ 0.70)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 · Calibration — Reliability Diagram\n",
    "\n",
    "ECE measures how well the model's confidence matches its actual accuracy per bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_flat = mean_np.flatten()\n",
    "targets_flat = msk_np.flatten()\n",
    "correct_flat = ((probs_flat >= 0.5) == (targets_flat > 0.5)).astype(float)\n",
    "\n",
    "ece, bin_accs, bin_confs, bin_freqs = expected_calibration_error(\n",
    "    probs_flat, correct_flat, n_bins=15\n",
    ")\n",
    "bs = brier_score(probs_flat, targets_flat)\n",
    "\n",
    "print(f\"Expected Calibration Error (ECE): {ece:.4f}\")\n",
    "print(f\"Brier Score                     : {bs:.4f}\")\n",
    "print(f\"(Target ECE < 0.03 for clinical deployment)\")\n",
    "\n",
    "reliability_diagram(\n",
    "    bin_accs,\n",
    "    bin_confs,\n",
    "    bin_freqs,\n",
    "    ece,\n",
    "    title=\"SAM2 Lung Nodule — Calibration (single slice)\",\n",
    "    save_path=\"reliability_diagram.png\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 · Threshold Sensitivity Analysis\n",
    "\n",
    "Different probability thresholds yield different Dice / precision / recall trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.dice_metric import compute_dice, compute_precision_recall\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 40)\n",
    "dices, precs, recs = [], [], []\n",
    "\n",
    "for t in thresholds:\n",
    "    d = compute_dice(mean_t, msk_t, threshold=float(t)).item()\n",
    "    p, r = compute_precision_recall(mean_t, msk_t, threshold=float(t))\n",
    "    dices.append(d)\n",
    "    precs.append(p.item())\n",
    "    recs.append(r.item())\n",
    "\n",
    "best_t = thresholds[np.argmax(dices)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(thresholds, dices, \"#4C72B0\", lw=2, label=\"Dice\")\n",
    "ax.plot(thresholds, precs, \"#55A868\", lw=2, ls=\"--\", label=\"Precision\")\n",
    "ax.plot(thresholds, recs, \"#C44E52\", lw=2, ls=\":\", label=\"Recall\")\n",
    "ax.axvline(0.5, color=\"gray\", ls=\":\", lw=1, label=\"Default (0.5)\")\n",
    "ax.axvline(best_t, color=\"#DD8452\", ls=\"--\", lw=2, label=f\"Best Dice t={best_t:.2f}\")\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Threshold Sensitivity Analysis\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0.1, 0.9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"threshold_sensitivity.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best threshold : {best_t:.2f}  →  Dice={max(dices):.4f}\")\n",
    "print(f\"Default (0.50) →  Dice={dices[np.argmin(np.abs(thresholds-0.5))]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}